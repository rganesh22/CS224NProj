{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "markdown-cell-1",
   "metadata": {},
   "source": [
    "# Baseline 1 (Direct regression on IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "code-cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertTokenizer, BertModel, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load IMDb dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "\n",
    "# Define tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Set format for PyTorch\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n",
    "\n",
    "# Convert labels to float\n",
    "def convert_labels_to_float(example):\n",
    "    example['label'] = float(example['label'])\n",
    "    return example\n",
    "\n",
    "tokenized_datasets = tokenized_datasets.map(convert_labels_to_float)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(tokenized_datasets['train'], shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(tokenized_datasets['test'], batch_size=batch_size)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "    for batch in train_progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['label'].float())\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    eval_loss = 0\n",
    "    eval_progress_bar = tqdm(eval_dataloader, desc=f\"Epoch {epoch+1} Evaluation\", leave=False)\n",
    "    for batch in eval_progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['label'].float())\n",
    "            eval_loss += outputs.loss.item()\n",
    "        logits = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "        label = batch['label'].cpu().numpy()\n",
    "        predictions.extend(logits)\n",
    "        labels.extend(label)\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Eval Loss: {avg_eval_loss:.4f}, Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a8bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"bert_imdb_regression.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_save_path = \"bert_tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"Tokenizer saved to {tokenizer_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a726e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_load_path = \"bert_tokenizer\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_load_path)\n",
    "\n",
    "# Load the model\n",
    "model_load_path = \"bert_imdb_regression.pt\"\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)\n",
    "model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "# Function to preprocess and predict sentiment score\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "    return prediction\n",
    "\n",
    "# Test the model with some example texts\n",
    "test_texts = [\n",
    "    \"This movie was fantastic! The plot was gripping and the characters were well-developed.\",\n",
    "    \"I didn't enjoy this film. The storyline was weak and the acting was subpar.\",\n",
    "    \"An average movie. Some good parts but also some very boring scenes.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    score = predict_sentiment(text)\n",
    "    print(f\"Text: {text}\\nPredicted Sentiment Score: {score}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9df01b",
   "metadata": {},
   "source": [
    "# Baseline 2: Contrastive Learning -> IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb3f030",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "print(\"Loading snli dataset\")\n",
    "# Load a different dataset for contrastive learning, e.g., SNLI\n",
    "snli_dataset = load_dataset('snli')\n",
    "# Extract the premises and hypotheses\n",
    "premises = snli_dataset['train']['premise']\n",
    "hypotheses = snli_dataset['train']['hypothesis']\n",
    "\n",
    "# Combine premises and hypotheses\n",
    "all_texts = premises + hypotheses\n",
    "\n",
    "# Get unique sentences\n",
    "contrastive_texts = list(set(all_texts))\n",
    "\n",
    "import pandas as pd\n",
    "contrastive_df = pd.DataFrame({'Contrastive Text': contrastive_texts})\n",
    "contrastive_df.to_csv('contrastive_texts.csv', index=False)\n",
    "\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the texts for contrastive learning\n",
    "contrastive_encodings = tokenizer(contrastive_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "contrastive_dataset = ContrastiveDataset(contrastive_encodings)\n",
    "\n",
    "# DataLoader for contrastive learning\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class BERTSimCSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSimCSE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "\n",
    "# SimCSE Loss function\n",
    "class SimCSELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(SimCSELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = self.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        # Create labels for contrastive loss\n",
    "        labels = torch.arange(batch_size).cuda()\n",
    "        labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "        # Mask to remove self-comparisons\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).cuda()\n",
    "\n",
    "        # Remove self-comparisons\n",
    "        similarity_matrix = similarity_matrix[~mask].view(labels.shape[0], -1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "# Training setup for contrastive learning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BERTSimCSE().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = SimCSELoss()\n",
    "\n",
    "print(\"Contrastive Learning Loop\")\n",
    "\n",
    "# Contrastive learning loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(contrastive_loader) // 10\n",
    "    for batch_idx, batch in enumerate(contrastive_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(input_ids, attention_mask)\n",
    "\n",
    "        # Apply dropout again and get another representation\n",
    "        model.bert.train()  # Ensure dropout is enabled\n",
    "        z_j = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(contrastive_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(contrastive_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(contrastive_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "print(\"Loading imdb dataset\")\n",
    "\n",
    "# Load the IMDB dataset for fine-tuning\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "train_texts = imdb_dataset['train']['text']\n",
    "train_labels = imdb_dataset['train']['label']\n",
    "test_texts = imdb_dataset['test']['text']\n",
    "test_labels = imdb_dataset['test']['label']\n",
    "\n",
    "# Tokenize the texts for IMDB\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDBDataset(train_encodings, train_labels)\n",
    "test_dataset = IMDBDataset(test_encodings, test_labels)\n",
    "\n",
    "# DataLoader for fine-tuning\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Fine-tuning setup\n",
    "class IMDBRatingModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(IMDBRatingModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "# Fine-tuning setup\n",
    "model = IMDBRatingModel(model.bert).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluation function to calculate MSE on the test set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "    print_interval = len(test_loader) // 10\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(test_loader):\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = elapsed_time / (batch_idx + 1) * (len(test_loader) - (batch_idx + 1))\n",
    "                print(f'Evaluating batch [{batch_idx+1}/{len(test_loader)}], Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n",
    "print(\"Fine Tuning Loop\")\n",
    "\n",
    "# Fine-tuning loop with additional metrics\n",
    "model.train()\n",
    "for epoch in range(3):  # Fine-tune for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(train_loader) // 10\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(train_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse = evaluate_model(model, test_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/3], Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec089f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"bert_contrastive_imdb_regression.pt\"\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer_save_path = \"bert_contrastive_tokenizer\"\n",
    "tokenizer.save_pretrained(tokenizer_save_path)\n",
    "print(f\"Tokenizer saved to {tokenizer_save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102bc86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer_load_path = \"bert_contrastive_tokenizer\"\n",
    "tokenizer = BertTokenizer.from_pretrained(tokenizer_load_path)\n",
    "\n",
    "# Load the model\n",
    "model_load_path = \"bert_contrastive_imdb_regression.pt\"\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "model.load_state_dict(torch.load(model_load_path))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to the appropriate device\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "# Function to preprocess and predict sentiment score\n",
    "def predict_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    prediction = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "    return prediction\n",
    "\n",
    "# Test the model with some example texts\n",
    "test_texts = [\n",
    "    \"This movie was fantastic! The plot was gripping and the characters were well-developed.\",\n",
    "    \"I didn't enjoy this film. The storyline was weak and the acting was subpar.\",\n",
    "    \"An average movie. Some good parts but also some very boring scenes.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    score = predict_sentiment(text)\n",
    "    print(f\"Text: {text}\\nPredicted Sentiment Score: {score}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237972a5",
   "metadata": {},
   "source": [
    "# Baseline 3 Directly train for Tamil movie review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962067b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Custom Dataset class to handle loading and processing\n",
    "class MovieReviewsDataset(Dataset):\n",
    "    def __init__(self, reviews, ratings, tokenizer, max_length):\n",
    "        self.reviews = reviews\n",
    "        self.ratings = ratings\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        review = self.reviews[idx]\n",
    "        rating = self.ratings[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            review,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(),\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(rating, dtype=torch.float)\n",
    "        }\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"tamil_movie_reviews_train.csv\")\n",
    "test_df = pd.read_csv(\"tamil_movie_reviews_test.csv\")\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts = train_df['ReviewInTamil'].tolist()\n",
    "train_labels = train_df['Rating'].tolist()\n",
    "test_texts = test_df['ReviewInTamil'].tolist()\n",
    "test_labels = test_df['Rating'].tolist()\n",
    "\n",
    "# Define tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-cased', num_labels=1)\n",
    "\n",
    "# Create datasets\n",
    "max_length = 512\n",
    "train_dataset = MovieReviewsDataset(train_texts, train_labels, tokenizer, max_length)\n",
    "test_dataset = MovieReviewsDataset(test_texts, test_labels, tokenizer, max_length)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n",
    "eval_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Optimizer and learning rate scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "num_epochs = 10\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1} Training\", leave=False)\n",
    "    for batch in train_progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['label'].float())\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_dataloader)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    eval_loss = 0\n",
    "    eval_progress_bar = tqdm(eval_dataloader, desc=f\"Epoch {epoch+1} Evaluation\", leave=False)\n",
    "    for batch in eval_progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=batch['input_ids'], attention_mask=batch['attention_mask'], labels=batch['label'].float())\n",
    "            eval_loss += outputs.loss.item()\n",
    "        logits = outputs.logits.squeeze(-1).cpu().numpy()\n",
    "        label = batch['label'].cpu().numpy()\n",
    "        predictions.extend(logits)\n",
    "        labels.extend(label)\n",
    "\n",
    "    avg_eval_loss = eval_loss / len(eval_dataloader)\n",
    "    mse = mean_squared_error(labels, predictions)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}, Eval Loss: {avg_eval_loss:.4f}, Mean Squared Error: {mse:.4f}\")\n",
    "\n",
    "print(\"Training complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa55e6a",
   "metadata": {},
   "source": [
    "# Baseline 4 Contrastive Learning with Tamil Murasu and then train on regression for ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ae29a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Load contrastive learning dataset\n",
    "print(\"Loading Tamil Murasu dataset\")\n",
    "tamil_murasu_df = pd.read_csv('tamilmurasu_dataset.csv')\n",
    "contrastive_texts = tamil_murasu_df['news_article'].tolist()\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize the texts for contrastive learning with progress statements\n",
    "print(\"Tokenizing texts for contrastive learning\")\n",
    "batch_size = 500\n",
    "total_texts = len(contrastive_texts)\n",
    "print_interval = total_texts // 10\n",
    "contrastive_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, total_texts, batch_size):\n",
    "    batch_texts = contrastive_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    contrastive_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    contrastive_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "    \n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (total_texts - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {total_texts} texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing texts for contrastive learning\")\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "contrastive_dataset = ContrastiveDataset(contrastive_encodings)\n",
    "\n",
    "# DataLoader for contrastive learning\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class BERTSimCSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSimCSE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "\n",
    "# SimCSE Loss function\n",
    "class SimCSELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(SimCSELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = self.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        # Create labels for contrastive loss\n",
    "        labels = torch.arange(batch_size).to(z_i.device)\n",
    "        labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "        # Mask to remove self-comparisons\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(z_i.device)\n",
    "\n",
    "        # Remove self-comparisons\n",
    "        similarity_matrix = similarity_matrix[~mask].view(labels.shape[0], -1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "# Training setup for contrastive learning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BERTSimCSE().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = SimCSELoss()\n",
    "\n",
    "print(\"Contrastive Learning Loop\")\n",
    "\n",
    "# Contrastive learning loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(contrastive_loader) // 10\n",
    "    for batch_idx, batch in enumerate(contrastive_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(input_ids, attention_mask)\n",
    "\n",
    "        # Apply dropout again and get another representation\n",
    "        model.bert.train()  # Ensure dropout is enabled\n",
    "        z_j = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(contrastive_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(contrastive_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(contrastive_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b9b5b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Loading Tamil movie reviews dataset\")\n",
    "\n",
    "# Load the Tamil movie reviews dataset for regression\n",
    "train_df = pd.read_csv(\"tamil_movie_reviews_train.csv\")\n",
    "test_df = pd.read_csv(\"tamil_movie_reviews_test.csv\")\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts = train_df['ReviewInTamil'].tolist()\n",
    "train_labels = train_df['Rating'].tolist()\n",
    "test_texts = test_df['ReviewInTamil'].tolist()\n",
    "test_labels = test_df['Rating'].tolist()\n",
    "\n",
    "# Tokenize the texts for regression with progress statements\n",
    "print(\"Tokenizing texts for regression\")\n",
    "train_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "test_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(train_texts), batch_size):\n",
    "    batch_texts = train_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    train_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    train_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(train_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(train_texts)} training texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing training texts for regression\")\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_texts), batch_size):\n",
    "    batch_texts = test_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    test_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    test_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(test_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(test_texts)} test texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing test texts for regression\")\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewsDataset(train_encodings, train_labels)\n",
    "test_dataset = ReviewsDataset(test_encodings, test_labels)\n",
    "\n",
    "# DataLoader for fine-tuning\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Fine-tuning setup\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        logits = self.regressor(cls_output)\n",
    "        return logits\n",
    "\n",
    "# Fine-tuning setup\n",
    "model = RegressionModel(model.bert).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ece1d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Evaluation function to calculate MSE on the test set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "    print_interval = len(test_loader) // 10 \n",
    "    if print_interval == 0:\n",
    "        print_interval = 1\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = outputs.squeeze()       \n",
    "        \n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        all_preds.extend(preds.cpu().detach().numpy())    \n",
    "        \n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(test_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(test_loader) - (batch_idx + 1))\n",
    "            print(f'Evaluating batch [{batch_idx+1}/{len(test_loader)}], Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    print(len(all_labels), len(all_preds))\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n",
    "print(\"Fine Tuning Loop\")\n",
    "\n",
    "# Fine-tuning loop with additional metrics\n",
    "model.train()\n",
    "for epoch in range(30):  # Fine-tune for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(train_loader) // 10\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(train_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse = evaluate_model(model, test_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/3], Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cde8848",
   "metadata": {},
   "source": [
    "# Baseline 5 Machine translated SNLI for contrastive learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3435836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from datasets import load_dataset\n",
    "# from googletrans import Translator\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "from translate import Translator\n",
    "import time\n",
    "\n",
    "def translate_texts(texts, target_language='ta'):\n",
    "    translator = Translator(to_lang=target_language)\n",
    "    translations = []\n",
    "    total_texts = len(texts)\n",
    "    print_interval = total_texts // 1000  # Print progress every 10%\n",
    "    start_time = time.time()\n",
    "\n",
    "    for i, text in enumerate(texts, start=1):\n",
    "        translation = translator.translate(text)\n",
    "        print(translation)\n",
    "        translations.append(translation)\n",
    "\n",
    "        if i % print_interval == 0 or i == total_texts:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            completed_percentage = (i / total_texts) * 100\n",
    "            remaining_time = (elapsed_time / i) * (total_texts - i)\n",
    "            print(f\"Translated {i}/{total_texts} texts ({completed_percentage:.2f}% complete), ETA: {remaining_time:.2f}s\")\n",
    "\n",
    "    return translations\n",
    "\n",
    "print(\"Loading SNLI dataset\")\n",
    "# Load the SNLI dataset\n",
    "snli_dataset = load_dataset('snli')\n",
    "english_texts = snli_dataset['train']['premise'] + snli_dataset['train']['hypothesis']\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with English texts\n",
    "english_df = pd.DataFrame({'English Text': english_texts})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "english_df.to_csv('snli_texts.csv', index=False)\n",
    "\n",
    "print(\"English texts saved to 'snli_texts.csv'\")\n",
    "\n",
    "print(\"Translating texts to Tamil\")\n",
    "tamil_texts = translate_texts(english_texts)\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize the texts for contrastive learning with progress statements\n",
    "print(\"Tokenizing texts for contrastive learning\")\n",
    "batch_size = 500\n",
    "total_texts = len(tamil_texts)\n",
    "print_interval = total_texts // 10\n",
    "contrastive_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, total_texts, batch_size):\n",
    "    batch_texts = tamil_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    contrastive_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    contrastive_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "    \n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (total_texts - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {total_texts} texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing texts for contrastive learning\")\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "contrastive_dataset = ContrastiveDataset(contrastive_encodings)\n",
    "\n",
    "# DataLoader for contrastive learning\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class BERTSimCSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSimCSE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "\n",
    "# SimCSE Loss function\n",
    "class SimCSELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(SimCSELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = self.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        # Create labels for contrastive loss\n",
    "        labels = torch.arange(batch_size).to(z_i.device)\n",
    "        labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "        # Mask to remove self-comparisons\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(z_i.device)\n",
    "\n",
    "        # Remove self-comparisons\n",
    "        similarity_matrix = similarity_matrix[~mask].view(labels.shape[0], -1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "# Training setup for contrastive learning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BERTSimCSE().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = SimCSELoss()\n",
    "\n",
    "print(\"Contrastive Learning Loop\")\n",
    "\n",
    "# Contrastive learning loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(contrastive_loader) // 10\n",
    "    for batch_idx, batch in enumerate(contrastive_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(input_ids, attention_mask)\n",
    "\n",
    "        # Apply dropout again and get another representation\n",
    "        model.bert.train()  # Ensure dropout is enabled\n",
    "        z_j = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(contrastive_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(contrastive_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(contrastive_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "print(\"Loading Tamil movie reviews dataset\")\n",
    "\n",
    "# Load the Tamil movie reviews dataset for regression\n",
    "train_df = pd.read_csv(\"tamil_movie_reviews_train.csv\")\n",
    "test_df = pd.read_csv(\"tamil_movie_reviews_test.csv\")\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts = train_df['ReviewInTamil'].tolist()\n",
    "train_labels = train_df['Rating'].tolist()\n",
    "test_texts = test_df['ReviewInTamil'].tolist()\n",
    "test_labels = test_df['Rating'].tolist()\n",
    "\n",
    "# Tokenize the texts for regression with progress statements\n",
    "print(\"Tokenizing texts for regression\")\n",
    "train_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "test_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(train_texts), batch_size):\n",
    "    batch_texts = train_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    train_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    train_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(train_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(train_texts)} training texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing training texts for regression\")\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_texts), batch_size):\n",
    "    batch_texts = test_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    test_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    test_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(test_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(test_texts)} test texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing test texts for regression\")\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewsDataset(train_encodings, train_labels)\n",
    "test_dataset = ReviewsDataset(test_encodings, test_labels)\n",
    "\n",
    "# DataLoader for fine-tuning\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Fine-tuning setup\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        logits = self.regressor(cls_output)\n",
    "        return logits\n",
    "\n",
    "# Fine-tuning setup\n",
    "model = RegressionModel(model.bert).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Evaluation function to calculate MSE on the test set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "    print_interval = len(test_loader) // 10\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = outputs.squeeze()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(test_loader):\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = elapsed_time / (batch_idx + 1) * (len(test_loader) - (batch_idx + 1))\n",
    "                print(f'Evaluating batch [{batch_idx+1}/{len(test_loader)}], Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n",
    "print(\"Fine Tuning Loop\")\n",
    "\n",
    "# Fine-tuning loop with additional metrics\n",
    "model.train()\n",
    "for epoch in range(3):  # Fine-tune for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(train_loader) // 10\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(train_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse = evaluate_model(model, test_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/3], Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe830ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2842a41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "\n",
    "# Create a Translator object\n",
    "translator = Translator(service_urls=['translate.googleapis.com'])\n",
    "\n",
    "# Define the sentence to be translated\n",
    "english_sentence = \"Hello, how are you?\"\n",
    "\n",
    "# Translate the sentence to Tamil\n",
    "translation = translator.translate(english_sentence, dest='ta')\n",
    "\n",
    "# Print the translated text\n",
    "print(\"English:\", english_sentence)\n",
    "print(\"Tamil:\", translation.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dce1f1",
   "metadata": {},
   "source": [
    "# Translate Contrastive Sentences Into Tamil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e90e6dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline with the Tamil Llama model\n",
    "pipe = pipeline(\"text-generation\", model=\"abhinand/tamil-llama-7b-base-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d240680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_to_tamil(sentences):\n",
    "    translated_sentences = []\n",
    "    for sentence in sentences:\n",
    "        result = pipe(sentence, max_length=512, num_return_sequences=1, do_sample=False)\n",
    "        print(result)\n",
    "        translated_text = result[0]['generated_text']\n",
    "        translated_sentences.append(translated_text)\n",
    "    return translated_sentences\n",
    "\n",
    "# List of sentences to translate\n",
    "sentences_to_translate = [\n",
    "    \"Translate to Tamil: The quick brown fox jumps over the lazy dog.\",\n",
    "    \"Translate to Tamil: A journey of a thousand miles begins with a single step.\",\n",
    "    \"Translate to Tamil: To be or not to be, that is the question.\"\n",
    "]\n",
    "\n",
    "# Translate the sentences\n",
    "translated_sentences = translate_to_tamil(sentences_to_translate)\n",
    "\n",
    "# Print the translated sentences\n",
    "for original, translated in zip(sentences_to_translate, translated_sentences):\n",
    "    print(f\"Original: {original}\\nTranslated: {translated}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056412bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline with the Tamil Llama model\n",
    "# model_name = \"abhinand/tamil-llama-7b-base-v0.1\"\n",
    "# pipe = pipeline(\"text-generation\", model=model_name)\n",
    "\n",
    "def generate_response(prompt):\n",
    "    response = pipe(prompt, max_length=150, num_return_sequences=1, do_sample=True)[0]['generated_text']\n",
    "    return response\n",
    "\n",
    "def chat():\n",
    "    print(\"You are now chatting with the model. Type 'exit' to end the conversation.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            print(\"Ending the conversation. Goodbye!\")\n",
    "            break\n",
    "        response = generate_response(user_input)\n",
    "        print(f\"Model: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    chat()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3c7342",
   "metadata": {},
   "source": [
    "# Create SNLI Paraphrasings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b277fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama\n",
    "import time\n",
    "\n",
    "# Initialize the model\n",
    "llm = Ollama(model=\"llama3\")\n",
    "\n",
    "def paraphrase_sentence(sentence):\n",
    "    prompt = f\"Give me a paraphrase of the following sentence \\\"{sentence}\\\" Your response should only contain the response\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "# Read the input CSV file\n",
    "input_file = \"contrastive_texts.csv\"\n",
    "output_file = \"paraphrased_texts.csv\"\n",
    "\n",
    "df = pd.read_csv(input_file)\n",
    "\n",
    "# Assuming the column containing the sentences to be paraphrased is named 'Contrastive Text'\n",
    "if 'Contrastive Text' not in df.columns:\n",
    "    raise ValueError(\"The input CSV file must contain a 'Contrastive Text' column.\")\n",
    "\n",
    "# Create a new column for the paraphrased sentences\n",
    "paraphrased_sentences = []\n",
    "total_sentences = len(df)\n",
    "progress_interval = 10  # Adjust this value based on your preference for progress updates\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for index, sentence in enumerate(df['Contrastive Text']):\n",
    "    paraphrased_sentence = paraphrase_sentence(sentence)\n",
    "    paraphrased_sentences.append(paraphrased_sentence)\n",
    "    \n",
    "    # Print progress with ETA\n",
    "    if (index + 1) % progress_interval == 0 or (index + 1) == total_sentences:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_sentence = elapsed_time / (index + 1)\n",
    "        sentences_left = total_sentences - (index + 1)\n",
    "        eta = sentences_left * avg_time_per_sentence\n",
    "        eta_minutes, eta_seconds = divmod(eta, 60)\n",
    "        print(f\"Processed {index + 1}/{total_sentences} sentences. ETA: {int(eta_minutes)}m {int(eta_seconds)}s\")\n",
    "\n",
    "# Add the paraphrased sentences to the DataFrame\n",
    "df['paraphrased_sentence'] = paraphrased_sentences\n",
    "\n",
    "# Save the result to a new CSV file\n",
    "df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Paraphrased sentences saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401c1a6f",
   "metadata": {},
   "source": [
    "# Train Contrasting Learning -> IMDB on dataset with incorporated paraphrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69e9a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "print(\"Loading paraphrased_texts.csv\")\n",
    "df = pd.read_csv('paraphrased_texts.csv')\n",
    "\n",
    "# Combine sentences from both columns, filter out any \" characters\n",
    "contrastive_texts = list(set(df['Contrastive Text'].tolist() + df['paraphrased_sentence'].tolist()))\n",
    "contrastive_texts = [text.replace('\"', '') for text in contrastive_texts]\n",
    "print(len(contrastive_texts))\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the texts for contrastive learning\n",
    "contrastive_encodings = tokenizer(contrastive_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "contrastive_dataset = ContrastiveDataset(contrastive_encodings)\n",
    "\n",
    "# DataLoader for contrastive learning\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Define the model\n",
    "class BERTSimCSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSimCSE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "\n",
    "# SimCSE Loss function\n",
    "class SimCSELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(SimCSELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = self.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        # Create labels for contrastive loss\n",
    "        labels = torch.arange(batch_size).cuda()\n",
    "        labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "        # Mask to remove self-comparisons\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).cuda()\n",
    "\n",
    "        # Remove self-comparisons\n",
    "        similarity_matrix = similarity_matrix[~mask].view(labels.shape[0], -1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "# Training setup for contrastive learning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BERTSimCSE().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = SimCSELoss()\n",
    "\n",
    "print(\"Contrastive Learning Loop\")\n",
    "\n",
    "# Contrastive learning loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(contrastive_loader) // 10\n",
    "    for batch_idx, batch in enumerate(contrastive_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(input_ids, attention_mask)\n",
    "\n",
    "        # Apply dropout again and get another representation\n",
    "        model.bert.train()  # Ensure dropout is enabled\n",
    "        z_j = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(contrastive_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(contrastive_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(contrastive_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "print(\"Loading imdb dataset\")\n",
    "\n",
    "# Load the IMDB dataset for fine-tuning\n",
    "imdb_dataset = load_dataset('imdb')\n",
    "train_texts = imdb_dataset['train']['text']\n",
    "train_labels = imdb_dataset['train']['label']\n",
    "test_texts = imdb_dataset['test']['text']\n",
    "test_labels = imdb_dataset['test']['label']\n",
    "\n",
    "# Tokenize the texts for IMDB\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = IMDBDataset(train_encodings, train_labels)\n",
    "test_dataset = IMDBDataset(test_encodings, test_labels)\n",
    "\n",
    "# DataLoader for fine-tuning\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Fine-tuning setup\n",
    "class IMDBRatingModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(IMDBRatingModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, 2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        logits = self.classifier(cls_output)\n",
    "        return logits\n",
    "    \n",
    "# Fine-tuning setup\n",
    "model = IMDBRatingModel(model.bert).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Evaluation function to calculate MSE on the test set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "    print_interval = len(test_loader) // 10\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, batch in enumerate(test_loader):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "            if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(test_loader):\n",
    "                elapsed_time = time.time() - start_time\n",
    "                remaining_time = elapsed_time / (batch_idx + 1) * (len(test_loader) - (batch_idx + 1))\n",
    "                print(f'Evaluating batch [{batch_idx+1}/{len(test_loader)}], Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n",
    "print(\"Fine Tuning Loop\")\n",
    "\n",
    "# Fine-tuning loop with additional metrics\n",
    "model.train()\n",
    "for epoch in range(3):  # Fine-tune for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(train_loader) // 10\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(train_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse = evaluate_model(model, test_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/3], Test MSE: {mse}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f10512",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langchain_community.llms import Ollama\n",
    "import time\n",
    "\n",
    "# Initialize the model\n",
    "llm = Ollama(model=\"tamil-llama\")\n",
    "\n",
    "def translate_text(text):\n",
    "    prompt = f\"       : \\\"{text}\\\"\"\n",
    "    response = llm.invoke(prompt)\n",
    "    return response\n",
    "\n",
    "# Read the input file\n",
    "input_file = \"contrastive_texts.csv\"\n",
    "output_file = \"translated_paraphrased_texts.csv\"\n",
    "\n",
    "with open(input_file, \"r\", encoding=\"utf-8\") as file:\n",
    "    paraphrased_texts = file.readlines()\n",
    "\n",
    "# Translate each paraphrased text\n",
    "translated_texts = []\n",
    "total_texts = len(paraphrased_texts)\n",
    "progress_interval = 10  # Adjust this value based on your preference for progress updates\n",
    "print_interval = 40  # Print original and translated sentences\n",
    "start_time = time.time()\n",
    "\n",
    "for index, text in enumerate(paraphrased_texts):\n",
    "    translated_text = translate_text(text)\n",
    "    translated_texts.append(translated_text)\n",
    "    \n",
    "    # Print original and translated sentences every 40 sentences\n",
    "    if (index + 1) % print_interval == 0 or (index + 1) == total_texts:\n",
    "        original_sentence = text.strip()\n",
    "        translation = translated_text.strip()\n",
    "        print(f\"Original: {original_sentence}\\nTranslation: {translation}\\n\")\n",
    "\n",
    "    # Print progress with ETA\n",
    "    if (index + 1) % progress_interval == 0 or (index + 1) == total_texts:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        avg_time_per_text = elapsed_time / (index + 1)\n",
    "        texts_left = total_texts - (index + 1)\n",
    "        eta = texts_left * avg_time_per_text\n",
    "        eta_minutes, eta_seconds = divmod(eta, 60)\n",
    "        print(f\"Processed {index + 1}/{total_texts} texts. ETA: {int(eta_minutes)}m {int(eta_seconds)}s\")\n",
    "\n",
    "# Write translated texts to a new file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "    file.writelines(translated_texts)\n",
    "\n",
    "print(f\"Translated texts saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cc9fc4",
   "metadata": {},
   "source": [
    "# Paraphrased Translations -> SimCSE -> Tamil Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222d722b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Load contrastive learning dataset\n",
    "print(\"Loading Tamil Paraphrased dataset\")\n",
    "contrastive_texts = []\n",
    "with open('translated_paraphrased_texts.csv', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        contrastive_texts.append(line.strip())\n",
    "contrastive_texts = contrastive_texts\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize the texts for contrastive learning with progress statements\n",
    "print(\"Tokenizing texts for contrastive learning\")\n",
    "batch_size = 500\n",
    "total_texts = len(contrastive_texts)\n",
    "print_interval = total_texts // 10\n",
    "contrastive_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, total_texts, batch_size):\n",
    "    batch_texts = contrastive_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    contrastive_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    contrastive_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "    \n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (total_texts - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {total_texts} texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing texts for contrastive learning\")\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "contrastive_dataset = ContrastiveDataset(contrastive_encodings)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    \n",
    "    # Pad sequences to the maximum length in this batch\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks_padded = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_masks_padded\n",
    "    }\n",
    "\n",
    "# DataLoader for contrastive learning\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define the model\n",
    "class BERTSimCSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSimCSE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "\n",
    "# SimCSE Loss function\n",
    "class SimCSELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(SimCSELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = self.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        # Create labels for contrastive loss\n",
    "        labels = torch.arange(batch_size).to(z_i.device)\n",
    "        labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "        # Mask to remove self-comparisons\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(z_i.device)\n",
    "\n",
    "        # Remove self-comparisons\n",
    "        similarity_matrix = similarity_matrix[~mask].view(labels.shape[0], -1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "# Training setup for contrastive learning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BERTSimCSE().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = SimCSELoss()\n",
    "\n",
    "print(\"Contrastive Learning Loop\")\n",
    "\n",
    "# Contrastive learning loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(contrastive_loader) // 10\n",
    "    for batch_idx, batch in enumerate(contrastive_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(input_ids, attention_mask)\n",
    "\n",
    "        # Apply dropout again and get another representation\n",
    "        model.bert.train()  # Ensure dropout is enabled\n",
    "        z_j = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(contrastive_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(contrastive_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(contrastive_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96b0302",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Loading Tamil movie reviews dataset\")\n",
    "\n",
    "# Load the Tamil movie reviews dataset for regression\n",
    "train_df = pd.read_csv(\"tamil_movie_reviews_train.csv\")\n",
    "test_df = pd.read_csv(\"tamil_movie_reviews_test.csv\")\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts = train_df['ReviewInTamil'].tolist()\n",
    "train_labels = train_df['Rating'].tolist()\n",
    "test_texts = test_df['ReviewInTamil'].tolist()\n",
    "test_labels = test_df['Rating'].tolist()\n",
    "\n",
    "# Tokenize the texts for regression with progress statements\n",
    "print(\"Tokenizing texts for regression\")\n",
    "train_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "test_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(train_texts), batch_size):\n",
    "    batch_texts = train_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    train_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    train_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(train_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(train_texts)} training texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing training texts for regression\")\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_texts), batch_size):\n",
    "    batch_texts = test_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    test_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    test_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(test_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(test_texts)} test texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing test texts for regression\")\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewsDataset(train_encodings, train_labels)\n",
    "test_dataset = ReviewsDataset(test_encodings, test_labels)\n",
    "\n",
    "# DataLoader for fine-tuning\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Fine-tuning setup\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        logits = self.regressor(cls_output)\n",
    "        return logits\n",
    "\n",
    "# Fine-tuning setup\n",
    "model = RegressionModel(model.bert).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Evaluation function to calculate MSE on the test set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "    print_interval = len(test_loader) // 10 \n",
    "    if print_interval == 0:\n",
    "        print_interval = 1\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = outputs.squeeze()       \n",
    "        \n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        all_preds.extend(preds.cpu().detach().numpy())    \n",
    "        \n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(test_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(test_loader) - (batch_idx + 1))\n",
    "            print(f'Evaluating batch [{batch_idx+1}/{len(test_loader)}], Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    print(len(all_labels), len(all_preds))\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n",
    "print(\"Fine Tuning Loop\")\n",
    "\n",
    "# Fine-tuning loop with additional metrics\n",
    "model.train()\n",
    "for epoch in range(30):  # Fine-tune for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(train_loader) // 10\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(train_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse = evaluate_model(model, test_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/3], Test MSE: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e2334",
   "metadata": {},
   "source": [
    "# Shorter Translations -> SimCSE -> Tamil Movie Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ee8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import time\n",
    "\n",
    "# Load contrastive learning dataset\n",
    "print(\"Loading Tamil Paraphrased dataset\")\n",
    "contrastive_texts = []\n",
    "with open('translated_paraphrased_texts.csv', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        contrastive_texts.append(line.strip())\n",
    "contrastive_texts = contrastive_texts\n",
    "\n",
    "# Define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "# Tokenize the texts for contrastive learning with progress statements\n",
    "print(\"Tokenizing texts for contrastive learning\")\n",
    "batch_size = 500\n",
    "total_texts = len(contrastive_texts)\n",
    "print_interval = total_texts // 10\n",
    "contrastive_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, total_texts, batch_size):\n",
    "    batch_texts = contrastive_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    contrastive_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    contrastive_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "    \n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (total_texts - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {total_texts} texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing texts for contrastive learning\")\n",
    "\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "contrastive_dataset = ContrastiveDataset(contrastive_encodings)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    \n",
    "    # Pad sequences to the maximum length in this batch\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_masks_padded = torch.nn.utils.rnn.pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids_padded,\n",
    "        'attention_mask': attention_masks_padded\n",
    "    }\n",
    "\n",
    "# DataLoader for contrastive learning\n",
    "contrastive_loader = DataLoader(contrastive_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Define the model\n",
    "class BERTSimCSE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERTSimCSE, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.pooler_output\n",
    "\n",
    "# SimCSE Loss function\n",
    "class SimCSELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.05):\n",
    "        super(SimCSELoss, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.cosine_similarity = nn.CosineSimilarity(dim=-1)\n",
    "\n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)\n",
    "        similarity_matrix = self.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0)) / self.temperature\n",
    "\n",
    "        # Create labels for contrastive loss\n",
    "        labels = torch.arange(batch_size).to(z_i.device)\n",
    "        labels = torch.cat([labels, labels], dim=0)\n",
    "\n",
    "        # Mask to remove self-comparisons\n",
    "        mask = torch.eye(labels.shape[0], dtype=torch.bool).to(z_i.device)\n",
    "\n",
    "        # Remove self-comparisons\n",
    "        similarity_matrix = similarity_matrix[~mask].view(labels.shape[0], -1)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = F.cross_entropy(similarity_matrix, labels)\n",
    "        return loss\n",
    "\n",
    "# Training setup for contrastive learning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = BERTSimCSE().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = SimCSELoss()\n",
    "\n",
    "print(\"Contrastive Learning Loop\")\n",
    "\n",
    "# Contrastive learning loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(contrastive_loader) // 10\n",
    "    for batch_idx, batch in enumerate(contrastive_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        z_i = model(input_ids, attention_mask)\n",
    "\n",
    "        # Apply dropout again and get another representation\n",
    "        model.bert.train()  # Ensure dropout is enabled\n",
    "        z_j = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(z_i, z_j)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(contrastive_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(contrastive_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(contrastive_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9545872e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading Tamil movie reviews dataset\")\n",
    "\n",
    "# Load the Tamil movie reviews dataset for regression\n",
    "train_df = pd.read_csv(\"tamil_movie_reviews_train.csv\")\n",
    "test_df = pd.read_csv(\"tamil_movie_reviews_test.csv\")\n",
    "\n",
    "# Extract text and labels\n",
    "train_texts = train_df['ReviewInTamil'].tolist()\n",
    "train_labels = train_df['Rating'].tolist()\n",
    "test_texts = test_df['ReviewInTamil'].tolist()\n",
    "test_labels = test_df['Rating'].tolist()\n",
    "\n",
    "# Tokenize the texts for regression with progress statements\n",
    "print(\"Tokenizing texts for regression\")\n",
    "train_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "test_encodings = {'input_ids': [], 'attention_mask': []}\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(train_texts), batch_size):\n",
    "    batch_texts = train_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    train_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    train_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(train_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(train_texts)} training texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing training texts for regression\")\n",
    "\n",
    "start_time = time.time()\n",
    "for i in range(0, len(test_texts), batch_size):\n",
    "    batch_texts = test_texts[i:i+batch_size]\n",
    "    encodings = tokenizer(batch_texts, truncation=True, padding=True, max_length=256)\n",
    "    test_encodings['input_ids'].extend(encodings['input_ids'])\n",
    "    test_encodings['attention_mask'].extend(encodings['attention_mask'])\n",
    "\n",
    "    if (i + len(batch_texts)) % print_interval < batch_size:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        remaining_time = elapsed_time / (i + len(batch_texts)) * (len(test_texts) - (i + len(batch_texts)))\n",
    "        print(f\"Processed {i + len(batch_texts)} / {len(test_texts)} test texts, Elapsed time: {elapsed_time:.2f}s, Estimated remaining time: {remaining_time:.2f}s\")\n",
    "\n",
    "print(\"Finished tokenizing test texts for regression\")\n",
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = ReviewsDataset(train_encodings, train_labels)\n",
    "test_dataset = ReviewsDataset(test_encodings, test_labels)\n",
    "\n",
    "# DataLoader for fine-tuning\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Fine-tuning setup\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.regressor = nn.Linear(self.bert.config.hidden_size, 1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.pooler_output\n",
    "        logits = self.regressor(cls_output)\n",
    "        return logits\n",
    "\n",
    "# Fine-tuning setup\n",
    "model = RegressionModel(model.bert).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Evaluation function to calculate MSE on the test set\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    start_time = time.time()\n",
    "    print_interval = len(test_loader) // 10 \n",
    "    if print_interval == 0:\n",
    "        print_interval = 1\n",
    "\n",
    "    for batch_idx, batch in enumerate(test_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        preds = outputs.squeeze()       \n",
    "        \n",
    "        all_labels.extend(labels.cpu().detach().numpy())\n",
    "        all_preds.extend(preds.cpu().detach().numpy())    \n",
    "        \n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(test_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(test_loader) - (batch_idx + 1))\n",
    "            print(f'Evaluating batch [{batch_idx+1}/{len(test_loader)}], Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    print(len(all_labels), len(all_preds))\n",
    "    mse = mean_squared_error(all_labels, all_preds)\n",
    "    return mse\n",
    "\n",
    "print(\"Fine Tuning Loop\")\n",
    "\n",
    "# Fine-tuning loop with additional metrics\n",
    "model.train()\n",
    "for epoch in range(30):  # Fine-tune for 3 epochs\n",
    "    start_time = time.time()\n",
    "    total_loss = 0\n",
    "    print_interval = len(train_loader) // 10\n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (batch_idx + 1) % print_interval == 0 or (batch_idx + 1) == len(train_loader):\n",
    "            elapsed_time = time.time() - start_time\n",
    "            remaining_time = elapsed_time / (batch_idx + 1) * (len(train_loader) - (batch_idx + 1))\n",
    "\n",
    "            print(f'Epoch [{epoch+1}/3], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {total_loss/(batch_idx+1):.4f}, '\n",
    "                  f'Elapsed time: {elapsed_time:.2f}s, Remaining time: {remaining_time:.2f}s')\n",
    "\n",
    "    # Evaluate the model on the test set\n",
    "    mse = evaluate_model(model, test_loader, device)\n",
    "    print(f'Epoch [{epoch+1}/3], Test MSE: {mse}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
